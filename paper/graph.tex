\section{Graph Algorithms in MapReduce}
\label{sec:graph}

We begin with a MapReduce procedure for creating large, sparse,
randomized graphs, since they are the input for the algorithms
discussed below.  R-MAT graphs ~\cite{RMAT} are recursively generated
graphs with power-law degree distributions.  They are commonly used to
represent web and social networks.  The user specifices six parameters
that define the graph: the number of vertices $N$ and edges $M$, and
four parameters $a$, $b$, $c$, $d$ that sum to 1.0 and are discussed
below.  The algorithm in Figure \ref{fig:rmat} generates $M$ unique
non-zero entries in a sparse $N \times N$ matrix $A$, where each entry
$A_{ij}$ represents an edge between graph vertices $(V_i,V_j)$.

\begin{figure}[htb]
 \begin{center}\fbox{\begin{minipage}{\textwidth} \begin{tabbing}
  xxxx\=xxxxxxxxxxxx\=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\= \kill

$M_{\rm remain} = M$ \\
while $M_{\rm remain} > 0$: \\
\> Map: \> Generate $M_{\rm remain}/P$ random edges on each processor \\
           \> \> output Key = $(i,j)$, Value = NULL \\
\> Collate \\
\> Reduce: \> Remove duplicate edges \\
           \> \> input Key = $(i,j)$, MultiValue = one or more NULLs \\
           \> \> output Key = $(i,j)$, Value = NULL \\
\> $M_{\rm remain} = M - N_{kv}$

  \end{tabbing}
 \end{minipage}}\end{center}

 \caption{MapReduce algorithm for R-MAT graph generation.}

 \label{fig:rmat}
\end{figure}

In the {\it map()} operation, each of $P$ processors generates a $1/P$
fraction of the desired edges.  A single random edge ($i,j$) is computed
recursively as follows.  Pick a random quadrant of the $A$ matrix with
relative probabilities $a$, $b$, $c$, and $d$.  Treat the chosen
quadrant as a sub-matrix and select a random quadrant within it, in
the same manner.  Repeat this process $n$ times where $N = 2^n$.  At
the end of the recursion, the final ``quadrant'' is non-zero matrix
element $A_{ij}$.

The {\it map()} will often generate some small number of duplicate edges.
The {\it collate()} and {\it reduce()} operations remove the duplicates.  The
entire map-collate-reduce sequence is repeated until the number of
resulting key/value pairs $N_{kv} = M$.  For reasonably sparse graphs
this typically takes only a few iterations.

Note that the degree distribution of vertices in the graph depends on
the choice of parameters $a$, $b$, $c$, $d$.  If one of the four
values is larger than the other three, a highly skewed distribution
results.  Variants of the above algorithm can be used when $N$ is not
a power-of-two, to generate graphs with weighted edges (assign a
numeric value to the $A_{ij}$ edge), graphs without self edges
(require $i \ne j$), or graphs with undirected edges (require $i < j$).
Or the general R-MAT matrix can be further processed by MapReduce
operations to meet these requirements.

By using more than one MapReduce object, we can improve the performance
of the R-MAT generation algorithm.  In the improved algorithm
(Figure~\ref{fig:rmat2}), we emit newly created edges into MapReduce object
$E_{new}$ and {\it aggregate()} the edges to processors.  
We then add those edges
to the MapReduce object $E_{old}$, containing all previously generated
unique edges; the edges of $E_{old}$ are already aggregated to processors
using the same mapping as $E_{new}$.  We can then use the {\it compress()}
function to reduce duplicates locally on each processor.
Performance improvements
in this algorithm result from the reduced number of KV pairs communicated in
{\it aggregate()},
compared to the collate operation in the original algorithm, 
particularly in later iterations of the generation where few
edges are created.

\begin{figure}[htb]
 \begin{center}\fbox{\begin{minipage}{\textwidth} \begin{tabbing}
  xxxx\=xxxxxxxxxxxx\=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\= \kill

$M_{\rm remain} = M$ \\
while $M_{\rm remain} > 0$: \\
\> Map $E_{new}$: \> Generate $M_{\rm remain}/P$ random edges on each processor \\
           \> \> output Key = $(i,j)$, Value = NULL \\
\> Aggregate $E_{new}$. \\
\> Add $E_{new}$ to $E_{old}$. \\
\> Compress $E_{old}$: Remove duplicate edges \\
            \> \> input Key = $(i,j)$, MultiValue = one or more NULLs \\
            \> \> output Key = $(i,j)$, Value = NULL \\
\> $M_{\rm remain} = M - N_{kv}$

  \end{tabbing}
 \end{minipage}}\end{center}

 \caption{Improved MapReduce algorithm for R-MAT graph generation.}

 \label{fig:rmat2}
\end{figure}

\subsection{PageRank}

The PageRank algorithm assings a relative numeric rank to each
vertex in a graph.
It models the web as a directed graph $G(V,E)$, with each vertex $V_i
\in V$ representing a web page and each edge $(V_i, V_j) \in E$
representing a hyperlink from $V_i$ to $V_j$.  The probability of
moving from $V_i$ to another vertex $V_j$ is $\alpha/d_{out}(V_i) +
(1-\alpha)/|V|$, where $\alpha$ is a user-defined parameter (usually
0.8-0.9), $d_{out}(V_i)$ is the outdegree of vertex $V_i$, and $|V|$ is
the cardinality of $V$.  The first term represents the probability of
following a given link on page $V_i$; the second represents the
probability of moving to a random page.  For pages with no outlinks,
the first term is $\alpha/|V|$, indicating equal likelihood to move to
any other page.  Equivalently, the graph can be represented by a
matrix $A$~\cite{LangvilleMeyer05a}, with matrix entries $A_{ij} =
\alpha/d_{out}(V_i)$ if vertex $V_i$ links to $V_j$.  The PageRank
algorithm, then, is essentially a power-method iteration in which the
dominating computation is matrix-vector multiplication $A^T x=y$,
where $x$ is the PageRank vector from the previous iteration.
The algorithm in Figure \ref{fig:pr} performs these
iterations.

\begin{figure}[htb]
 \begin{center}\fbox{\begin{minipage}{\textwidth} \begin{tabbing}
  xxxx\=xxxxx\=xxxxx\=xxxxx\=xxxxxxxxxxxxxxxxxxxxxxxxxxx\= \kill

%Input from R-MAT generator: \\
%\> MapReduce object $V$ containing all graph vertices:  Key=$V_i$, Value = NULL.  \\
%\> MapReduce object $E$ containing all graph edges: Key=$V_i$, Value = $V_j$. \\
%Initialize MapReduce object for matrix $A$  and \\
%MapReduce object $MT$ containing all vertices with no outlinks.\\
%\> Copy: $A = E$ \\
%\> Add: $V$ to $A$ \\
%\> Collate $A$: Vertex is Key \\
%\> Reduce $A$:  \\
%\> \> Input:  Key = $V_i$; MultiValue = NULL,$V_j$,$V_k$,... \\
%\> \> If (${nvalues} > 1$) \\
%\> \> \> Compute: $d_{out}(V_i) = {nvalues} - 1$. \\
%\> \> \> Output:  For each $V_j$ in MultiValue, Key = $V_i$; Value = ($V_j$, $\alpha/d_{out}(V_i)$). \\
%\> \> else \\
%\> \> \> Output to $MT$: Key = $V_i$; Value = 0. \\
%Initialize MapReduce object for pagerank vector $x$. \\
%\> Map:   \\
%\> \> Input:  Graph vertices $V$. \\
%\> \> Output:  For each vertex, Key = $V_i$; Value = $1/|V|$. \\
Input: \\
\> MapReduce object $A$ containing $|V| \times |V|$ matrix:  Key = $i$, Value = $[j, a_{ij}]$ \\
\> MapReduce object $MT$ containing indices of empty rows of $A$:  Key = $i$, Value = 0. \\
\> MapReduce object $x$ containing initial PageRank vector:  Key = $i$, Value = $1/|V|$. \\
While (${residual} > {tolerance}$) \\
\> Compute contribution for random jumps and zero-outdegree vertices. \\
\> \> Add: $x$ to $MT$. \\
\> \> Collate $MT$:  Vertex is Key. \\
\> \> Reduce $MT$:   \\
\> \> \> Input:  Key = $i$; MultiValue = $x_i, 0$ if $i \in {MT}$; MultiValue = $x_i$ if $i \notin {MT}$. \\
\> \> \> Compute:  Local contributions $cont$. \\
\> \> \> Output:  If ${nvalues} = 2$, Key = $i$; Value = 0. \\
\> \> MPI\_Allreduce with MPI\_SUM to compute global contribution $gcont$. \\
\> Compute $y = A^T x$.  \\
\> \> Copy $x$ to $y$. \\
\> \> Add $A$ to $y$. \\
\> \> Collate $y$:  Key is row index of $A$. \\
\> \> Reduce $y$: \\
\> \> \>  Input: Key = $i$; Multivalue = $i, [j, a_{ij}]$ for nonzeros $a_{ij}$ in $A$ \\
\> \> \>  Output:  Key = $j$; Value = $a_{ij} y_i$. \\
\> \> Collate $y$:  Key is column index of $A$. \\
\> \> Reduce $y$:  \\
\> \> \> Input:  Key = $j$; Multivalue = $a_{ij} y_i$ for all non-zero entries $a_{ij}$ of column $j$ \\
\> \> \> Output:  Key = $j$; Value = $\sum_i a_{ij} y_i$. \\
\> Add contribution $gcont$ to $y$; compute local max norm $norm$ of $y$. \\
\> \> Map $y$:  \\
\> \> \> Input:  Key = $i$, Value = $y_i$. \\
\> \> \> Compute: $y_i = y_i + gcont$; if $y_i > {norm}$, $norm = y_i$. \\
\> \> \> Output:  Key = $i$, Value = $y_i$. \\
\> \> MPI\_Allreduce $norm$ with MPI\_MAX to compute global max norm $gnorm$. \\
\> Scale $y$ by $gnorm$. \\
\> \> Map $y$: \\
\> \> \> Input:  Key = $i$, Value = $y_i$. \\
\> \> \> Output:  Key = $i$, Value = $y_i / {gnorm}$. \\
\> Compute residual. \\
\> \> Add $y$ to $x$. \\
\> \> Collate $x$:  Key is index $i$. \\
\> \> Reduce $x$:   \\
\> \> \> Input:  Key = $i$; Multivalue = $x_i, y_i$. \\
\> \> \> Compute:  if $|x_i - y_i| > {resid}$, ${resid} = |x_i - y_i|$ \\
\> \> MPI\_Allreduce with MPI\_MAX to compute $residual$. \\
\> $x = y$; \\
Output:  PageRank vector $x$.
  \end{tabbing}
 \end{minipage}}\end{center}

 \caption{MapReduce algorithm for PageRank vertex ranking.}

 \label{fig:pr}
\end{figure}

%The MapReduce implementation performs two {\it map()} operations to
%initialize the graph matrix $A$ and PageRank vector $x$.  
The key kernel of PageRank is the matrix-vector multiplication $y = A^T x$.
In it, a {\it
collate()} operation gathers all row entries $a_{ij}$ with their
associated $x_i$ entry, and a {\it reduce()} computes $a_{ij} x_i$.  A
second {\it collate()} gathers, for each $j$, all contributions to the
column sum $\sum_i a_{ij} x_i$, which is computed by a second {\it
reduce()}.  MPI\_Allreduce calls are used to compute global norms and
residuals.

The entire PageRank algorithm can be made more efficient by calling
{\it aggregate()} on the MapReduce objects representing the matrix $A$, 
the empty matrix rows $MT$, and the vector $x$ once before the PageRank 
iterations begin.  Since these three MapReduce
objects have the same key types, pre-aggregating the objects moves all
Key-Value pairs with a given key to a single processor.  Thus, many of
the PageRank operations become local operations, replacing 
the {\it collate()} operation (which is equivalent to {\it aggregate()} 
followed by {\it convert()}) with only a local {\it convert()} call. 
Figure~\ref{fig:pr2} shows the
improved algorithm; modified operations are marked with an asterisk.
This strategy of pre-aggregating to improve data locality is useful in
many MapReduce algorithms, but assumes that keys are always mapped to
the same processor.

\begin{figure}[htb]
 \begin{center}\fbox{\begin{minipage}{\textwidth} \begin{tabbing}
  xxxx\=xxxxx\=xxxxx\=xxxxx\=xxxxxxxxxxxxxxxxxxxxxxxxxxx\= \kill

Input: \\
$*$\> Pre-aggregated MapReduce object $A$ containing $|V| \times |V|$ matrix:  Key = $i$, Value = $[j, a_{ij}]$ \\
$*$\> Pre-aggregated MapReduce object $MT$ containing indices of empty rows of $A$:  Key = $i$, Value = 0. \\
$*$\> Pre-aggregated MapReduce object $x$ containing initial PageRank vector:  Key = $i$, Value = $1/|V|$. \\
While (${residual} > {tolerance}$) \\
\> Compute contribution for random jumps and zero-outdegree vertices. \\
\> \> Add: $x$ to $MT$. \\
$*$\> \> Convert $MT$:  Vertex is Key. \\
\> \> Reduce $MT$:   \\
\> \> \> Input:  Key = $i$; MultiValue = $x_i, 0$ if $i \in {MT}$; MultiValue = $x_i$ if $i \notin {MT}$. \\
\> \> \> Compute:  Local contributions $cont$. \\
\> \> \> Output:  If ${nvalues} = 2$, Key = $i$; Value = 0. \\
\> \> MPI\_Allreduce with MPI\_SUM to compute global contribution $gcont$. \\
\> Compute $y = A^T x$.  \\
\> \> Copy $x$ to $y$. \\
\> \> Add $A$ to $y$. \\
$*$\> \> Convert $y$:  Key is row index of $A$. \\
\> \> Reduce $y$: \\
\> \> \>  Input: Key = $i$; Multivalue = $i, [j, a_{ij}]$ for nonzeros $a_{ij}$ in $A$ \\
\> \> \>  Output:  Key = $j$; Value = $a_{ij} y_i$. \\
\> \> Collate $y$:  Key is column index of $A$. \\
\> \> Reduce $y$:  \\
\> \> \> Input:  Key = $j$; Multivalue = $a_{ij} y_i$ for all non-zero entries $a_{ij}$ of column $j$ \\
\> \> \> Output:  Key = $j$; Value = $\sum_i a_{ij} y_i$. \\
\> Add contribution $gcont$ to $y$; compute local max norm $norm$ of $y$. \\
\> \> Map $y$:  \\
\> \> \> Input:  Key = $i$, Value = $y_i$. \\
\> \> \> Compute: $y_i = y_i + gcont$; if $y_i > {norm}$, $norm = y_i$. \\
\> \> \> Output:  Key = $i$, Value = $y_i$. \\
\> \> MPI\_Allreduce $norm$ with MPI\_MAX to compute global max norm $gnorm$. \\
\> Scale $y$ by $gnorm$. \\
\> \> Map $y$: \\
\> \> \> Input:  Key = $i$, Value = $y_i$. \\
\> \> \> Output:  Key = $i$, Value = $y_i / {gnorm}$. \\
\> Compute residual. \\
\> \> Add $y$ to $x$. \\
$*$\> \> Convert $x$:  Key is index $i$. \\
\> \> Reduce $x$:   \\
\> \> \> Input:  Key = $i$; Multivalue = $x_i, y_i$. \\
\> \> \> Compute:  if $|x_i - y_i| > {resid}$, ${resid} = |x_i - y_i|$ \\
\> \> MPI\_Allreduce with MPI\_MAX to compute $residual$. \\
\> $x = y$; \\
Output:  PageRank vector $x$.
  \end{tabbing}
 \end{minipage}}\end{center}

 \caption{Improved MapReduce algorithm for PageRank vertex ranking; pre-aggregating the MapReduce objects allows many communication intensive {\it collate()} 
operations to be replaced by with strictly local {\it convert()} operations (as marked by the asterisks).}

 \label{fig:pr2}
\end{figure}


\subsection{Triangle enumeration}

A triangle in a graph is any triplet of vertices $(V_i,V_j,V_k)$ where
the edges $(V_i,V_j), (V_j,V_k), (V_i,V_k)$ exist.  Figure
\ref{fig:tri} outlines a MapReduce algorithm that enumerates all
triangles, assuming an input graph of undirected edges $(V_i,V_j)$
where $V_i < V_j$ for every edge, i.e. an upper-triangular R-MAT
matrix.  This exposition follows the triangle-finding algorithm
presented in \cite{Cohen}.

\begin{figure}[htb]
 \begin{center}\fbox{\begin{minipage}{\textwidth} \begin{tabbing}
  xxxxxxxxxxxx\=xxx\=xxx\=xxx\=xxxxxxxxxxxxxxxxxxxxxxxxxxxx\= \kill

1 Copy: \> $G_0$ = copy of edge KV pairs from input graph \\
1 Map: \> Convert edges to vertices \\
           \> \> input Key = $(V_i,V_j)$, Value = NULL \\
           \> \> output Key = $V_i$, Value = $V_j$ \\
           \> \> output Key = $V_j$, Value = $V_i$ \\
1 Collate \\
1 Reduce: \> Add first degree to one vertex in edge \\
              \> \> input Key = $V_i$, MultiValue = $(V_j, V_k, ...)$ \\
	      \> \> for each V in MultiValue: \\
              \> \> \> if $V_i < V$: output Key = $(V_i,V)$, Value = $(D_i,0)$ \\
              \> \> \> else: output Key = $(V,V_i)$, Value = $(0,D_i)$ \\
2 Collate \\
2 Reduce: \> Add second degree to other vertex in edge \\
              \> \> input Key = $(V_i,V_j)$, MultiValue = $((D_i,0),(0,D_j))$ \\
              \> \> output Key = $(V_i,V_j)$, Value = $(D_i,D_j)$ with $V_i < V_j$ \\
3 Map: \> Low degree vertex emits edges \\
           \> \> if $D_i < D_j$: output Key = $V_i$, Value = $V_j$ \\
           \> \> else if $D_j < D_i$: output Key = $V_j$, Value = $V_i$ \\
           \> \> else: output Key = $V_i$, Value = $V_j$ \\
3 Collate \\
3 Reduce: \> Emit angles of each vertex \\
              \> \> input Key = $V_i$, MultiValue = $(V_j, V_k, ...)$ \\
	      \> \> for each $V_1$ in MultiValue: \\
	      \> \> \> for each $V_2$ beyond $V_1$ in MultiValue: \\
	      \> \> \> \> if $V_1 < V_2$: output Key = $(V_1,V_2)$, Value = $V_i$ \\
	      \> \> \> \> else: output Key = $(V_2,V_1)$, Value = $V_i$ \\
4 Add: \> Add $G_0$ edge KV pairs to angle KV pairs \\
4 Collate \\
4 Reduce: \> Emit triangles \\
              \> \> input Key = $(V_i,V_j)$, MultiValue = $(V_k,V_l,NULL,V_m,...)$ \\
              \> \> if NULL exists in MultiValue: \\
	      \> \> \> for each non-NULL V in MultiValue: \\
	      \> \> \> \> output Key = $(V_i,V_j,V)$, Value = NULL

  \end{tabbing}
 \end{minipage}}\end{center}

 \caption{MapReduce algorithm for triangle enumeration.}

 \label{fig:tri}
\end{figure}

The initial step is to store a copy of the graph edges as key/value
(KV) pairs in an auxiliary MapReduce object $G_0$, for use later in the
algorithm.  The first {\it map()} operation converts edge keys to
vertex keys with edge values.  After the {\it collate()}, each vertex
has a list of vertices it is connected to; the first {\it reduce()}
can thus flag one vertex $V_i$ in each edge with a degree count $D_i$.
The second {\it collate()} and {\it reduce()} assign a degree count
$D_j$ to the other vertex in each edge.  In the third {\it map()},
only the lower-degree vertex in each edge emits its edges as key/value
(KV) pairs.  The task of the third {\it reduce()} is to emit
``angles'' for each of these low-degree vertices.  An ``angle'' is a
root vertex $V_i$, with two edges to vertices $V_1$ and $V_2$, i.e. a
triangle without the third edge $(V_1,V_2)$.  The {\it reduce()} emits
a list of all angles of vertex $V_i$, by a double loop over the edges
of $V_i$.  Note that the aggregate volume of KV pairs emitted at this
stage is minimized by having only the low-degree vertex in each edge
generate angles.

In stage 4, the KV pairs in the original graph $G_0$ are added to the
current working set of KV pairs.  The KV pairs in $G_0$ are edges that
complete triangles for the angle KV pairs just generated.  After the
fourth {\it collate()}, a pair of vertices $(V_i,V_j)$ is the key, and
the multivalue is the list of all root vertices in angles that contain
$V_i$ and $V_j$.  If the multivalue also contains a NULL, contributed
by $G_0$, then there is a $(V_i,V_j)$ edge in the graph.  Thus all
vertices in the multivalue are roots of angles which are complete
triangles and can be emitted as a triplet key.

\subsection{Connected component labeling}

A connected component of a graph is a set of vertices where all pairs
of vertices in the set are connected by a path of edges.  A sparse
graph may contain many such components.  Figure \ref{fig:cc} outlines
a MapReduce algorithm that labels each vertex in a graph with a
component ID.  All vertices in the same component are labelled with
the same ID, which is the ID of a vertex in the component.  We assume
an input graph of undirected edges $(V_i,V_j)$.  This exposition also
follows the connected-component algorithm presented in \cite{Cohen},
with the addition of logic that load-balances data across processors
when one or a few giant components exist in the graph.

\begin{figure}[htb]
 \begin{center}\fbox{\begin{minipage}{\textwidth} \begin{tabbing}
  xxxx\=xxxxxxxxxxx\=xxxx\=xxxx\=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\= \kill

Iterate: \\
\> 1 Map: \> Convert edges to vertices \\
    \> \> \> input Key = $E_{ij}$, Value = NULL \\
    \> \> \> output Key = $V_i$, Value = $E_{ij}$ \\
    \> \> \> output Key = $V_j$, Value = $E_{ij}$ \\
\> 1 Add: \> Zone assignment of each vertex \\
    \> \> \> output Key = $V_i$, Value = $Z_i$ \\
\> 1 Collate: \> Vertex as key \\
\> 1 Reduce: \> Emit edges of each vertex with zone of vertex \\
       \> \> \> input Key = $V_i$, MultiValue = $E E E E ... Z$ \\
       \> \> \> for each $E$ in MultiValue: \\
      \> \> \> \> output Key = $E_{ij}$, Value = $Z_i$ \\

\> 2 Collate: \> Edge as key \\
\> 2 Reduce: \> Emit zone re-assignments \\
       \> \> \> input Key = $E_{ij}$, MultiValue = $Z_i Z_j$ \\
       \> \> \> $Z_{winner}$ = min($Z_i$,$Z_j$); $Z_{loser}$ = max($Z_i$,$Z_j$) \\
       \> \> \> if $Z_i$ and $Z_j$ are different: \\
      \> \> \> \> output Key = $Z_{loser}$, Value = $Z_{winner}$ \\
\> 2 Exit: \> if no output by Reduce 2 \\
\> 3 Map: \> Invert vertex/zone pairs \\
    \> \> \> input Key = $V_i$, Value = $Z_i$ \\
    \> \> \> if $Z_i$ is not partitioned: \\
   \> \> \> \> output Key = $Z_i$, Value = $V_i$ \\
    \> \> \> else: \\
   \> \> \> \> output Key = $Z_i^+$, Value = $V_i$ for a random processor \\
\> 3 Add: \> Changed zones ($Z_i$,$Z_{winner}$) \\
    \> \> \> if $Z_i$ is not partitioned: \\
   \> \> \> \> output Key = $Z_i$, Value = $Z_{winner}$ \\
    \> \> \> else: \\
   \> \> \> \> output Key = $Z_i^+$, Value = $Z_{winner}$ for every processor \\
   \> \> \> \> output Key = $Z_i$, Value = $Z_{winner}$ \\
\> 3 Collate: \> Zone ID as key \\
\> 3 Reduce: \> Emit new zone assignment of each vertex \\
       \> \> \> input Key = $Z_i$ or $Z_i^+$, MultiValue = $V V V V ... Z Z Z ...$ \\
       \> \> \> $Z_{new}$ = min($Z_i$ or $Z_i^+$,Z,Z,Z,...) \\
       \> \> \> partition $Z_{new}$ if number of $V >$ threshhold \\
       \> \> \> for each $V$ in MultiValue: \\
      \> \> \> \> output Key = $V_i$, Value = $Z_{new}$

  \end{tabbing}
 \end{minipage}}\end{center}

 \caption{MapReduce algorithm for connected component labeling.}

 \label{fig:cc}
\end{figure}

The algorithm begins (before the iteration loop) by assigning each
vertex to its own component or ``zone'', so that $Z_i$ = $V_i$.  Each
iteration will grow the zones, one layer of neighbors at a time.  As
zones collide due to shared edges, a winner is chosen (the smaller
zone ID), and vertices in the losing zone are reassigned to the
winning zone.  When the iterations complete, each zone will have
become a fully connected component.  The algorithm thus finds all
connected components in the graph simultaneously.  The number of
iterations required depends on the largest diameter of any component
in the graph.

The first {\it map()} operation emits the vertices in each edge as
keys, with the edge as a value.  The current zone assignment of each
vertex is added to the set of key/value pairs.  The first {\it
collate()} operation collects all the edges of a vertex and its zone
assignment together in one multi-value.  The first {\it reduce()}
operation then re-emits each edge, tagged by the zone assignment of
one of its vertices.

Since each edge was emitted twice, the second {\it collate()}
operation collects the zone assignments for its two vertices together.
If the two zone IDs are different, the second {\it reduce()} operation
chooses a winner (the min of the 2 IDs), and emits the loser ID as a
key, with the winning ID as a value.  If no zone ID changes are
emitted, the algorithm is finished, and the iteration exits.

The third {\it map()} operation inverts the vertex/zone key/value
pairs to become zone/vertex pairs.  The third {\it add()} operation
adds the changing zone assignments to the set of key/value pairs.  The
third collate() can then collects all the vertices of a zone and zero
or more reassignments for the zone ID.  Since a zone could collide
with multiple other zones on the same iteration due to shared edges,
the new zone ID becomes the minimum ID of any of the neighboring
zones.  If no zone reassignment value appears in the multi-value, the
zone ID is unchanged.  The final reduce() a key/value pair for each
vertex in the zone, with the vertex as a key and the new zone ID as a
value.

Note that if a graph has only a few components, then the third {\it
collate()} operation, which keys on the zone ID, may generate a few
very large key/multi-value (KMV) pairs.  For example, if the graph is
fully connected, then on the last iteration, a single KMV pair will
contain all vertices in the graph and be assigned to one processor.
This imbalance in memory and computational work can lead to poor
parallel performance of the overall algorithm.  To counter this
effect, the various operations of stage 3 include extra logic.  The
idea is to partition zones whose vertex count exceeds a user-defined
threshhold into $P$ sub-zones, where $P$ is the number of processors.
The 64-bit integer that stores the zone ID also stores a bit flag
indicating the zone has been partitioned and a set of bits that encode
the processor ID.

During the third {\it map()} operation, if the zone has been
partitioned, then the vertex is assigned to a random processor and the
processor ID bits are added to the zone ID, as indicated by the
$Z_i^+$ notation in Figure \ref{fig:cc}.  Likewise, if $Z_i$ has been
partitioned, the third {\it add()} operation emits the zone ID change
key/value pair ($Z_i$,$Z_{winner}$) as ($Z_i^+$,$Z_{winner}$).  In
this case ($Z_i$,$Z_{winner}$), is emitted not once, but $P+1$ times,
once for each processor, and once as if $Z_i$ had not been partitioned
(for a reason discussed below).

This additional partitioning logic means that the third {\it
collate()} operation, which keys on zone IDs, some of which now
include processor bits, will collect only a $1/P$ subset of the
vertices in large zones onto each processor.  But the multivalue on
each processor contains all the zone-reassignments relevant to the
unpartitioned zone.  This allows the third {\it reduce()} operation to
change the zone ID (if necessary) in a consistent manner across all
$P$ multi-values that contain the zone's vertices.  When the {\it
reduce()} operation emits new zone assignments for each vertex, the
zone retains its partitioned status, and the partition bit is also
explicitly set if the vertex count exceeds the threshhold for the
first time.

Note that this logic does not guarantee that the partition bits of the
zone IDs for all the vertices in a single zone will be set
consistently on a given iteration.  For example, an unpartitioned zone
with a small ID may consume a partitioned zone.  The vertices from the
partitioned zone will retain their partitioned status, but the
original vertices in the small zone may not set the partition bit of
their zone IDs.  On subsequent iterations, the third {\it add()}
operation emits $P+1$ copies of new zone reassignments for both
partitioned and unpartioned zone IDs, to insure all vertices in the
zone will know the reassignment information.

\subsection{Maximal independent set identification}

An ``independent'' set of vertices in a graph is one where no pair of
vertices in the set share an edge.  The set is ``maximal'' if no
additional vertex can be added \footnote{A maximal set is ``maximum''
if it is the largest maximal set.  Finding a graph's maximum
independent set is an NP-hard problem, which MapReduce is unlikely to
help with.}.  Finding a maximal independent set (MIS) is useful in
several contexts, such as identifying large numbers of indepenent
starting vertices for graph traversals.  Luby's algorithm \cite{Luby}
is a well-known parallel method for finding a MIS.  Figure
\ref{fig:luby} outlines a MapReduce version of Luby's algorithm,
assuming an input graph of undirected edges $(V_i,V_j)$ where $V_i <
V_j$ for every edge, i.e. an upper-triangular R-MAT matrix.

\begin{figure}[htb]
 \begin{center}\fbox{\begin{minipage}{\textwidth} \begin{tabbing}
  xxxx\=xxxxxxxxxxx\=xxxx\=xxxx\=xxxx\=xxxxxxxxxxxxxxxxxxxxxxxxx\= \kill

Map: assign random values to each vertex \\
Clone: convert edge key/values directly to key/multivalues \\
Create: empty MapReduce MRv for maximal independent set vertices \\
\\
while $N_{edges} > 0$: \\
\> 1 Reduce: \> Determine WINNER/LOSER vertex of each edge \\
       \> \> \> input Key = $E_{ij}$ with $V_i < V_j$, Multivalue = NULL or LOSER \\
       \> \> \> if either value is LOSER, emit nothing \\
       \> \> \> $V_w$ = WINNER vertex, $V_l$ = LOSER vertex \\
       \> \> \> output Key = $V_w$, Value = $V_l$ WINNER \\
       \> \> \> output Key = $V_l$, Value = $V_w$ LOSER \\
\> 1 Collate: \> Vertex as key \\
\> 2 Reduce: \> Find WINNER vertices \\
       \> \> \> input Key = $V_i$, MultiValue = $V V V V ...$ \\
       \> \> \> if all $V$ are WINNERs: \\
      \> \> \> \> for each $V$ in Multivalue: \\
     \> \> \> \> \> output Key = $V_j$, Value = $Vi$ LOSER \\
       \> \> \> else: \\
      \> \> \> \> for each $V$ in Multivalue: \\
     \> \> \> \> \> output Key = $V_j$, Value = $V_i$ \\
\> 2 Collate: \> Vertex as key \\
\> 3 Reduce: \> Find LOSER vertices \\
       \> \> \> input Key = $V_i$, MultiValue = $V V V V ...$ \\
       \> \> \> if any $V$ is LOSER: \\
      \> \> \> \> for each $V$ in Multivalue: \\
     \> \> \> \> \> output Key = $V_j$, Value = $Vi$ LOSER \\
       \> \> \> else: \\
      \> \> \> \> for each $V$ in Multivalue: \\
     \> \> \> \> \> output Key = $V_j$, Value = $V_i$ \\
\> 3 Collate: \> Vertex as key \\
\> 4 Reduce: \> Emit WINNER vertices and LOSER edges \\
       \> \> \> input Key = $V_i$, MultiValue = $V V V V ...$ \\
       \> \> \> if all $V$ are LOSERs: \\
      \> \> \> \> output to MRv, Key = $V_i$, Value = NULL \\
       \> \> \> for each $V$ in Multivalue: \\
      \> \> \> \> if $V$ is LOSER: \\
     \> \> \> \> \> output Key = $E_{ij}$ with $V_i < V_j$, Value = LOSER \\
      \> \> \> \> else: \\
     \> \> \> \> \> output Key = $E_{ij}$ with $V_i < V_j$, Value = NULL \\
\> 4 Collate: \> Edge as key \\

  \end{tabbing}
 \end{minipage}}\end{center}

 \caption{MapReduce algorithm for finding a maximal independent set of
 graph vertices via Luby's algorithm.}

 \label{fig:luby}
\end{figure}

Before the iterations begin, a random value is assigned to each
vertex, which will be used to compare pairs of vertices.  The vertex
ID itself could be used as the random value, but since the vertices
eventually selected for the MIS depend on the randomization, this may
introduce an unwanted bias.  Instead a random number generator can be
used to assign a consistent random value $R_i$ to each vertex in each
edge (via a map() operation), which is then carried along with the
vertex ID through each stage of the algorithm.  In the notation of
Figure \ref{fig:luby}, each $V_i$ is then really two quantities, a
vertex ID (1 to $N$) and $R_i$.  Alternatively, the $R_i$ can be used
to relabel the vertices in a random manner, assigning each a new,
unique vertex ID from 1 to $N$.  In this case, $V_i$ is simply the new
vertex ID, and the vertices in the final MIS could be remapped to
recover their original IDs.  These operations could be performed with
a few extra MapReduce steps, outside the iteration loop.

The pre-iteration clone() operation converts the initial list of edge
key/values one by one to key/multivalue pairs, with the edge as the
key, and NULL as the value.  This is so the iterations can begin with
a reduce() operation, without need for a collate() operation.  A
second empty MapReduce object is also initialized, which will
accumulate MIS vertices as the iterations proceed.

At each iteration, the current set of edges are examined to identify
winning vertices.  Vertices adjacent to winners are flagged as losers,
and all edges adjacent to losers are flagged for removal at the start
of the next iteration.  Vertices with all losing edges are flagged as
winners.  At each iteration edges are removed from the graph, and
winning vertices are added to the MIS.  When the graph is empty, the
MIS is complete.

The first reduce() operation flags the two vertices in each edge as a
potential winner or loser.  This is done by comparing the two random
values for the vertices (or the randomized vertex IDs as explained
above).  The first collate() operation thus produces a multivalue for
each vertex $V_i$ which contains all the vertices it shares an edge
with, and associated winner/loser flags.  In the second reduce()
operation, if $V_i$ won the comparison with all its edge neighbors,
then it is a ``winner'' vertex.  All vertices connected to the winner
are emitted with a loser flag.  If $V_i$ is not an overall winner, its
edge vertices are emitted without the loser flag.  The second
collate() operation collects this new information for each vertex.

In the third reduce() operation, if $V_i$ was flagged as a loser by
any edge neighbor who was a winner, then it becomes a losing vertex
and emits all its edge vertices with a loser flag.  Otherwise it emits
all its edge vertices without the flag.

In the fourth reduce() operation, some vertices will have all their
edge vertices flagged as losers.  These vertices are all winners,
either the ones identified in the second reduce() operation, or
additional vertices who would become singletons (having no edges) when
edges containing a loser vertex are removed (on the next iteration).
All of these winning vertices are emitted to the MapReduce object that
stores the accumulating MIS.

All edges of each vertex are also emitted, retaining the loser flag if
they have it.  They are emitted with $E_ij$ as their key, with $V_i <
V_j$.  This is to insure both copies of the edge come together via the
fourth collate() operation.  The multivalue for each edge will now
have two values, either of which may be NULL or a loser flag.

At the first stage of the next iteration, any edge flagged by either
value as a loser does not compare its two vertices; it is effectively
deleted from the graph.  The Luby iterations end when all edges have
been removed from the graph.  At this point, the second MapReduce
object contains a complete MIS of vertices.

\subsection{Single source shortest path}

The single-source shortest path (SSSP) algorithm computes, for a directed graph
with weighted edges,
the shortest weighted distance from a given source vertex to
all other vertices in the graph.  This operation is straightforward
when global graph information is available.  The source vertex is labeled with
distance zero.  Then in each iteration, edge adjacencies of labeled vertices
are followed, and adjacent vertices are labeled with updated distances.  When
no distances change, the iterations are complete.  Most importantly, only
edges from modified vertices are visited in each iteration.

In MapReduce, global information about edge adjacencies and labeled vertices
is not available.  Instead, following the Bellman-Ford-style 
algorithm of~\cite{SSSPMapReduce, BellmanFord},
every edge of the graph is visited in each iteration,
although labels for an edge's vertices are updated only if the breadth-first
traversal has reached those vertices.  The algorithm begins with a {\it map()}
function loading edges $(V_i, V_j)$ and source vertex $V_s$ with distance 0.
In each iteration, the edges $(V_i, V_j)$ are collated with respect to
$V_i$, and a {\it reduce()} operation updates each $V_j$'s distance if
$V_i$'s distance is non-zero.  The algorithm is fully described in
Figure~\ref{fig:sssp}.


\begin{figure}[htb]
 \begin{center}\fbox{\begin{minipage}{\textwidth} \begin{tabbing}
  xxxx\=xxxx\=xxxxxx\=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\= \kill

Input:  \\
\> Source vertex $V_s$.\\
\> MapReduce object $V$ containing graph vertices $V_i$ with distances $d_i$ from $V_s$ (initially $\infty$):  \\
\> \> Key = $V_i$; Value = $\infty$.\\
\> MapReduce object $E$ containing graph edges ($V_i, V_j$) with weights $w_{ij}$: \\
\> \>  Key = $V_i$; Value = $[V_j, w_{ij}]$ \\
Map $V$:  Set distance for source vertex $V_s$ to zero. \\
\> Input:  Key = $V_i$; Value = $\infty$ \\
\> Output:  Key = $V_i$; Value = 0 if $V_i = V_s$, otherwise Value = $\infty$. \\
while not done: \\
\> done = 1; \\
\> Add $E$ to $V$. \\
\> Collate $V$:  Key is vertex $V_i$. \\
\> Reduce $V$:  Loop over all edges to perform breadth-first search from labeled vertices. \\
\> \> Input:  Key = $V_i$; Multivalue = edges $[V_j, w_{ij}]$, candidate distances $d_i$. \\
\> \> Compute:  Find smallest candidate distance $d_{min}$ for $V_i$.  \\
\> \> \> If $d_{min} \ne d_i$, done = 0; \\
\> \> Output:  Key = $V_i$; Value = $d_{min}$ (updated distance for $V_i$) \\
\> \> \> and if $d_{min} \ne d_i$, Key = $V_j$; Value = $d_{min} + w_{ij}$ for each edge ($V_i, V_j$). \\
\> MPI\_Allreduce on done with MPI\_MIN. \\
Output: MapReduce object $V$ with Key = $V_i$, Value = $d_i$. 


  \end{tabbing}
 \end{minipage}}\end{center}

 \caption{MapReduce algorithm for single-source shortest path (SSSP)
 determination.}

 \label{fig:sssp}
\end{figure}

As with PageRank and R-MAT generation, 
significant savings in communication and execution time
can be achieved by pre-aggregating certain MapReduce objects and 
storing data in more than one MapReduce object.  The improved algorithm
is shown in Figure~\ref{fig:sssp2}.  The MapReduce objects storing
vertices and edges are aggregated to processors only once at the beginning
of the computation.  Then, in each iteration of 
SSSP, only a small number of new candidate distances are generated.
The improved algorithm uses MapReduce object $U$ to store these updates.
Only the updates are communicated; each update is aggregated to the 
same processor that stores the corresponding vertex and its edges.
Communication is performed using $U$, so the total amount of communication
needed is small compared to the original algorithm (which aggregates all
graph edges in each iteration).  Updated vertex distances are stored back
in the the MapReduce object for vertices.

\begin{figure}[htb]
 \begin{center}\fbox{\begin{minipage}{\textwidth} \begin{tabbing}
  xxxx\=xxxx\=xxxx\=xxxx\=xxxx\=xxxxxxxxxxxxxxxxxxxxxxxxxx\= \kill

Input:  \\
\> Source vertex $V_s$.\\
\> Pre-aggregated MapReduce object $V$ containing graph vertices $V_i$ with distances $d_i$ from $V_s$:  \\
\> \> Key = $V_i$; Value = $\infty$.\\
\> Pre-aggregated MapReduce object $E$ containing graph edges ($V_i, V_j$) with weights $w_{ij}$: \\
\> \>  Key = $V_i$; Value = $[V_j, w_{ij}]$ \\
$U$ = new MapReduce object to store updates to distances. \\
Map $U$:  Add $V_s$ with distance $d_s = 0$ to $U$. \\
\> Input:  $V_s$ \\
\> Output:  Key = $V_s$; Value = 0. \\
while not done: \\
\> done = 1; \\
\> Aggregate $U$. \\
\> Add $U$ to $V$. Empty $U$.\\
\> Compress $V$:  Pick best candidate distance for each vertex. \\
\> \> Input:  Key = $V_i$; Multivalue = candidate distances $d_i$. \\
\> \> Compute:  Find smallest candidate distance $d_{min}$ for $V_i$.  \\
\> \> \> If $d_{min} \ne d_i$, done = 0; \\
\> \> Output to $V$:  Key = $V_i$; Value = $d_{min}$. \\
\> \> Output to $U$: if $d_{min} \ne d_i$, Key = $V_i$; Value = $d_{min}$. \\
\> MPI\_Allreduce on done with MPI\_MIN. \\
\> if not done \\
\> \> Add $U$ to $E$.  Empty $U$. \\
\> \> Compress $E$:  Generate candidate distances for neighbors of changed vertices. \\
\> \> \> Input:  Key = $V_i$; Multivalue = $[V_j, w_{ij}]$ for edges ($V_i, V_j$) \\
\> \> \> \> and distance $d_i$ if updated in previous step. \\
\> \> \> Output to $U$:  If updated distance $d_i$ exists, \\
\> \> \> \> Key = $V_j$; Value = $d_i+w_{ij}$ for each edge ($V_i, V_j$). \\
Output: MapReduce object $V$ with Key = $V_i$, Value = $d_i$. 

  \end{tabbing}
 \end{minipage}}\end{center}

 \caption{Improved MapReduce algorithm for single-source shortest path (SSSP)
 determination.  Pre-aggregating the vertices and edges, and later aggregating only updates to vertex distances reduces communication and execution time.}

 \label{fig:sssp2}
\end{figure}

