\section{MapReduce in MPI}
\label{sec:mr}

The basic datums stored and operated on by any MapReduce framweork are
key/value (KV) pairs.  In the MR-MPI library, individual keys or
values can be of any data type or length, or combinations of multiple
types (one integer, a string of characters, two integers and a double,
etc); they are simply treated as byte strings by the library.  A KV
pair always has a key; a KV's value may be NULL.  A related data type
is the key/multivalue (KMV) pair, where all values associated with the
same key are collected and stored contiguously as a multivalue, which
is just a longer byte string with an associated vector of lengths, one
integer length per value.  In this section, we assume the datums
operated on by each processor fit in local memory.

A typical MR-MPI program makes at least three calls to the MR-MPI
library, to perform {\it map()}, {\it collate()}, and {\it reduce()}
operations.  In a {\it map()}, zero or more key/value pairs are
generated by each processor.  Often this is done using data read from
files, but a {\it map()} may generate data itself or process existing
KV pairs to create new ones.  The KV pairs produced are stored locally
by each processor; a {\it map()} thus requires no inter-processor
communication.  Users call the library with a count of tasks to
perform and a pointer to a user function; the MR-MPI {\it map()}
operation invokes the user function multiple times as a callback.
Depending on which variant of {\it map()} is called, the user function
may be passed a file name, a chunk of bytes from a large file, a task
ID, or a KV pair.  Options for assigning map tasks to processors are
specified by the user and include assigning consecutive chunks of
tasks to processors, striding the tasks across processors, or using a
master-slave model that is useful when tasks have widely varying
workloads.

The {\it collate()} operation (or data shuffle in Hadoop) identifies
unique keys and collects all the values associated with those keys to
create KMV pairs.  This is done in two stages, the first of which
requires communication, since KV pairs with the same key may be owned
by any processor.  Each processor hashes each of its keys to determine
which processor will ``own'' it.  The $k$-byte length key is hashed
into a 32-bit value whose remainder modulo $P$ processors is the
owning processor rank.  Alternatively, the user can provide a hash
function which converts the key into a processor rank.  Each processor
then sends each of its KV pairs to the owning processor.

After receiving new KV pairs, the second stage is an on-processor
computation, requiring no further communication.  Each processor
reorganizes its KV pairs into KMV pairs, one for each unique key it
owns.  This is done using a hash table, rather than a sort.  A sort
scales as $N\log_2(N)$, i.e. it requires $\log_2(N)$ passes through
the $N$ KV pairs.  With hashing, the list of KMV pairs can be created
in two passes.  The first pass populates the hash table with needed
count and length information; the second pass copies the key and value
datums into the appropriate location in a new KMV data structure.
Since the cost to lookup a key in a well-formed hash table is a
constant-time $O(1)$ operation, the cost of the data reorganization is
also $O(N)$.  This methodology has the added benefit that the number
of values in each KMV pair is known and can be passed to the user
function during a reduce.  For some reduce operations, this count is
all the information a reduce requires; the values need not be looped
over.  The count is not available in Hadoop-style data shuffles, which
sort the values; counting the values associated with the same key
requires iterating over the values.

Note that the first portion of the {\it collate()} operation involves
all-to-all communication (each processor sends and receives data from
every other processor) using a distributed hash table.  The
communication can either be done via a MPI\_Alltoall() library call,
or by a custom routine that aggregates messages and invokes
point-to-point MPI\_Send() and MPI\_IRecv() calls.

The {\it reduce()} operation processes KMV pairs and can produce new
KV pairs for continued computation.  Each processor operates only on
the KMV pairs it owns; no communication is required.  As with the {\it
map()}, users call the library with a pointer to a user function.  The
MR-MPI {\it reduce()} operation invokes the user function, once for
each KMV pair.

Several related MapReduce operations are provided by the library.  For
example, the {\it collate()} function described above calls two other
functions: {\it aggregate()}, which performs the all-to-all
communication, and {\it convert()}, which turns a list of KV pairs
into KMV pairs.  Both functions can be called directly.  The {\it
compress()} function allows on-processor operations to be performed;
it is equivalent to a {\it convert()} with on-processor KVs as input,
followed by a {\it reduce()}.  The {\it clone()} function turns a list
of KV pairs into KMV pairs, with one value per key.  The {\it
collapse()} function turns $N$ KV pairs into one KMV pair, with the
keys and values of the KV pairs becoming $2N$ values of a single
multivalue assigned to a new key.  The {\it gather()} function
collects KV pairs from all processors to a subset of processors; it is
useful for doing output from one or a few processors.  Library calls
for sorting datums by key or value or for sorting the values within
each multivalue are also provided.  These routines invoke the
C-library {\it quicksort()} function to compute the sorted ordering of
the KV pairs (or values within a multivalue), using a user-provided
comparison function.  The KV pairs (or values in a multivalue) are
then copied into a new data structure in sorted order.

The interface to various low-level operations is provided so
that a user's program can string them together in various ways to
produce interesting MapReduce algorithms.  For example, output from a
{\it reduce()} can serve as input to a subsequent {\it map()} or {\it
collate()}.  KV pairs from multiple MapReduce objects can be combined
to perform new sequences of {\it map()}, {\it collate()}, and
{\it reduce()} operations.

The above discussion assumed that the KV or KMV pairs stored by a
processor fit in its physical memory.  In this case, MR-MPI performs
only ``in-core'' processing and no disk files are written or read by
any of the processors, aside from initial input data if it exists or
final output data if it is generated.  Note that the aggregate
physical memory of large parallel machines can be multiple terabytes,
which allows for large data sets to be processed in-core, assuming the
KV and KMV pairs remain evenly distributed across processors
throughout the sequence of MapReduce operations.  The use of hashing
to assign keys to processors typically provides for such
load-balancing.  In the next section, we discuss what happens when
data sets do not fit in available memory.
