\section{MapReduce in MPI}
\label{sec:mr}

The basic datums stored and operated on by any MapReduce framweork are
key/value (KV) pairs.  In the MR-MPI library, individual keys or
values can be of any data type or length, or combinations of multiple
types (one integer, a string of characters, two integers and a double,
etc); they are simply treated as byte strings by the library.  A KV
pair always has a key; a KV's value may be NULL.  A related data type is
the key/multivalue (KMV) pair, where all values associated with the
same key are collected and stored contiguously as a multivalue, which
is just a longer byte string with an associated vector of integer
lengths, one per value.

A typical MR-MPI program makes at least three calls to the MR-MPI library,
to perform a {\it map()}, {\it collate()}, and {\it reduce()}
operation.  In a {\it map()}, zero or more key/value pairs are
generated by each processor.  Typically, this is done using data read
from files, but a {\it map()} may generate data itself or process
existing KV pairs to create new ones.  The KV pairs produced are
stored locally by each processor; a {\it map()} thus requires no
inter-processor communication.  Users call the library with a count of
tasks to perform and a pointer to a user function; the MR-MPI {\it
map()} operation invokes the user function multiple times as a
callback.  Depending on which variant of {\it map()} is called, the
user function may be passed a file name, a chunk of bytes from a large
file, a task ID, or a KV pair.  Options for assigning map tasks to processors
are specified by the user and include assigning consecutive 
chunks of tasks to processors, striding the tasks across processors,
or using a master-slave model that is useful when tasks have widely
varying workloads.

The {\it collate()} operation (or data shuffle in Hadoop) identifies
unique keys and collects all the values associated with those keys to
create KMV pairs.  This is done in two stages, the first of which
requires communication, since KV pairs with the same key may be owned
by any processor.  Each processor hashes each of its keys to determine
which processor will ``own'' it.  The $k$-byte length key is hashed into
a 32-bit value whose remainder modulo $P$ processors generates the owning
processor rank.  Alternatively, the user can provide a hash function
which converts the key into a processor rank.  Each processor then sends
each of its KV pairs to the owning processor.

After receiving new KV pairs, the second stage is an on-processor
computation, requiring no further communication.  Each processor
reorganizes its KV pairs into KMV pairs, one for each unique key it
owns.  This is done using a hash table, rather than a sort.  A sort
requires $\log(N)$ passes through the $N$ KV pairs.  With hashing,
the list of KMV
pairs can be created in two passes through the KV pairs. The first 
pass populates
the hash table with needed count and length information; the
second pass copies the key and value datums into the appropriate
location in a new KMV data structure.  Since the lookup of a key in a
well-formed hash table is a constant-time $O(1)$ operation, the cost
of the data reorganization is $O(N)$, rather than $O(N\log(N))$ for full
sorting.  It also has the added benefit that the number of values in each
KMV pair is known and is passed to the user function during a
reduce.  For some reduce operations, this count is all the information a
reduce requires; thus the values need not be looped over.  This feature is not
available in Hadoop-style data reorganizations, which sort the values; 
with sorting, the number of values associated with the same key is not known 
without interating over the values.

Note that the first portion of the {\it collate()} operation involves
all-to-all communication (each processor sends and receives data from
every other processor) driven by a distributed hash table.  The
communication can either be done via a MPI\_Alltoall() library call,
or by a custom routine that aggregates messages and invokes pointwise
MPI\_Send() and MPI\_IRecv() calls.

The {\it reduce()} operation processes KMV pairs and can produce new
KV pairs for continued computation.  Each processor operates only on
the KMV pairs it owns; no communication is required.  As with the {\it
map()}, users call the library with a pointer to a user function.  The
MR-MPI {\it reduce()} operation invokes the user function, once for
each KMV pair.

Several related MapReduce operations are provided by the library and
exposed in the library interface.  For example, the {\it collate()} function
described above
calls two other functions:  {\it aggregate()}, which performs the all-to-all
communication, and {\it convert()}, which turns a list of KV pairs into
KMV pairs.  
Both {\it aggregate()} and {\it convert()} are exposed in the MR-MPI
interface for direct access by the user.
The {\it compress()} function allows on-processor operations to be 
performed; it is equivalent to a {\it convert()} with on-processor KVs as
input, followed by a {\it reduce()}.  
The {\it clone()} operation turns a list of KV pairs into
KMV pairs, with one value per key.  The {\it collapse()} operation turns
$N$ KV pairs into one KMV pair, with the keys and values of the KV
pairs becoming $2N$ values of a single multivalue assigned to a new
key on the KMV.  The {\it gather()} operation collects KV pairs from
all processors to a subset of processors; it is useful for doing
output from a single or subset of processors.  Library calls for
sorting datums by key or value or within multivalues are also
provided.  These routines invoke the C-library {\it quicksort()} function to
compute the sorted ordering of the KV pairs (or values within a
multivalue), using a user-provided comparison function.  The KV pairs
(or values in a multivalue) are then copied one-by-one into a new data
structure in sorted order.

The interface to various low-level operations is provided so
that a user's program can string them together in various ways to
produce interesting MapReduce algorithms.  For example, output from a
{\it reduce()} can serve as input to a subsequent {\it map()} or {\it
collate()}.  KV pairs from multiple MapReduce objects can be combined
to perform new sequences of {\it map()}, {\it collate()}, and
{\it reduce()} operations.

The above discussion assumed that the KV or KMV pairs stored by a
processor fit in its physical memory.  In this case, MR-MPI 
performs only ``in-core'' processing and no disk files are written
or read by any of the processors, aside from initial input data if it
exists or final output data if it is generated.  Note that the
aggregate physical memory of large parallel machines can be multiple
terabytes, which allows for large data sets to be processed in-core,
assuming the KV and KMV pairs remain evenly distributed across
processors throughout the sequence of MapReduce operations.  The use
of hashing to assign keys to processors is designed for
such load-balancing.  In the next section, we discuss what happens 
when data sets do not fit in available memory.
