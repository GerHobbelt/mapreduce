\section{Performance Results}
\label{sec:results}

In this section, we present performance results for the MapReduce
graph algorithms of the preceeding section, implemented as small C++
programs calling our MR-MPI library.
The benchmarks were run on a medium-sized Linux cluster made up of 2 GHz
dual-core AMD Opteron processors with a Myrinet network.  Most importantly
for MR-MPI, each node of the cluster has one local disk, which is used
for out-of-core operations in MR-MPI.  To avoid contention for disk I/O, we 
ran all experiments with one MPI process per node.  For comparisons with
other implementations, we used either the same cluster or, where noted,
Sandia's Cray XMT, a multi-threaded parallel computer with 500 {MHz} processors
and a 3D-Torus network.

We ran each of the algorithms on four R-MAT graphs of
different sizes, each on a varying number of processors.  Details of the
input data are shown in Table~\ref{t:rmats}.
The {\it small} problem size (around 8M edges) can typically be used
on a single processor without significant out-of-core operations.
The {\it medium} problem size (around 134M edges) is a 
problem that can be run
on a single processor with out-of-core operations; larger processor 
configurations, however, do not necessarily need out-of-core operations.
The {\it large} problem size (around 2B
edges) requires out-of-core operations and higher processor counts.
%The {\it x-large} problem size
%(around 34B edges) requires most of the machine to run.
All data sets used R-MAT parameters $(a, b, c, d) = (0.57, 0.19, 0.19, 0.05)$.

\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
Data & \# of    & \# of & Maximum \\
Set  & vertices & edges & vertex degree\\
\hline
RMAT-20 (small)   &$2^{20} \approx 1M$ & $2^{23} \approx 8M$ &  $\approx 24K$ \\
RMAT-24 (medium)  &$2^{24} \approx 17M$ & $2^{27} \approx 134M$ &  $\approx 147K$ \\
RMAT-28 (large)   &$2^{28} \approx 268M$ & $2^{31} \approx 2B$& $\approx 880K$ \\
%RMAT-32 (x-large) &$2^{32} \approx 4B$ & $2^{35} \approx 34B$ &   \\
\hline
\end{tabular}
\caption{Characteristics of R-MAT input data for graph algorithm
experiments.}
\label{t:rmats}
\end{center}
\end{table}

The resulting timings give a sense of the inherent scalability of the
MapReduce algorithms as graph size grows on a fixed number of
processors, and of the parallel scalability for computing on a graph
of fixed size on a growing number of processors.  Where available, we
compare the MapReduce algorithm with other parallel implementations.
We compute parallel efficiency on $p$ processors
as $({time}_{m} \times m) / ({time}_p \times p)$
where $m$ is the smallest number of processors on which the experiment
was run, and ${time}_m$ is the execution time required on $m$ processors.

\subsection{R-MAT generation results}

In Figure~\ref{f:rmat}, we show the scalability of the R-MAT generation 
algorithm (Figure~\ref{fig:rmat2}) for RMAT-20, RMAT-24 and RMAT-28.  For
RMAT-20 and RMAT-24, superlinear speed-up is shown.  This speed-up is due
to the decreased amount of file I/O needed with greater numbers of 
processors; with a larger total memory, a fixed-size problem requires
less I/O with more processors.  For RMAT-28, where most operation require
significant out-of-core operation, parallel efficiency ranges from 52\% to
97\%.  Comparisons between the enhanced algorithm (Figure~\ref{fig:rmat2}) and
the original algorithm (Figure~\ref{fig:rmat}) showed approximately 
10\% reduction in execution time for the enhanced algorithm.

KDD NOTE:  Add XMT or PBGL results here.

\begin{figure}[htb]
\includegraphics[width=\textwidth]{fig_rmat.pdf}
\caption{Performance of R-MAT generation algorithm (Figure~\ref{fig:rmat2}).}
\label{f:rmat}
\end{figure}

\subsection{PageRank results}

In Figure~\ref{f:pr}, we show the performance of the MR-MPI PageRank algorithm
(Figure~\ref{fig:pr2}) compared to a
distributed-memory matrix-based implementation using the linear
algebra toolkit Trilinos~\cite{Trilinos-Overview} 
and a multi-threaded
implementation
in the Multi-Threaded Graph Library (MTGL)~\cite{MTGL}.
The matrix-based distributed-memory implementation of PageRank
uses Trilinos' Epetra matrix/vector classes to represent the graph and
PageRank vector.
Rows of matrix $A$ and the associated entries of the PageRank vector $x$
are uniquely assigned to processors; a random permutation of the input
matrix achieves processor load balance.
Interprocessor communication gathers $x$ values for matrix-vector
multiplication and sums partial products into the $y$ vector.
Most communication is point-to-point communication,
but some global communication is needed for computing
residuals and norms of $x$ and $y$.
In the multi-threaded MTGL~\cite{MTGL} implementation,
rank propagates via adjacency list traversal
in a compressed sparse-row data structure.
To maintain scalability, code must
be written so that a single thread spawns the loop that processes all
in-neighbors of a given vertex; this detail enables the compiler to generate
hotspot-free code.

Figure~\ref{f:pr} shows the execution time per PageRank iteration for
R-MAT matrices RMAT-20, RMAT-24 and RMAT-28.  
Converging the PageRank iterations
to tolerance $0.002$ requires five or six iterations.
Several R-MAT matrices of 
each size were generated for the experiments; the average time over the
matrices is reported here.  
The Trilinos implementations show near-perfect
strong scaling for RMAT-20 and RMAT-24.  The MR-MPI 
implementations also demonstrate good strong scaling.  However, MR-MPI's
execution time is at least an order of magnitude greater than the Trilinos
implementation.  This result is due to two factors:  ({\it i}) a 
higher volume of
communication in the MapReduce implementation (where all edges are communicated
in each iteration), compared to the Trilinos implementation (where only
vertex-based data are communicated), and ({\it ii}) out-of-core operations
in MR-MPI were performed because of the page-size restriction, while all
Trilinos operations were performed in-core.  The benefit of MR-MPI's
out-of-core implementation is seen, however, with the RMAT-28 data set, 
which could be solved on smaller processor sets than the Trilinos 
implementation.  For these experiments, the Trilinos implementation required
64 processors for the RMAT-28 data set. 

KDD NOTE:  Need to add XMT results; PBGL would be nice, too.


\begin{figure}[htb]
\includegraphics[width=\textwidth]{fig_pagerank.pdf}
\caption{Comparison of PageRank implementations using MapReduce,
Trilinos, and MTGL on R-MAT data sets.}
\label{f:pr}
\end{figure}


\subsection{Triangle finding results}

In Figure~\ref{f:tri}, we show the performance of the triangle finding 
algorithm (Figure~\ref{fig:tri}).  Execution times for this algorithm were
too large to allow the problem to be run with RMAT-28 on our cluster.
Parallel efficiencies for RMAT-20 ranged from 80\% to 140\%; for RMAT-24, they
ranged from 50\% to 120\%.

KDD NOTE:  Need to add XMT results; triangle enumeration is not implemented in PBGL.

\begin{figure}[htb]
\includegraphics[width=\textwidth]{fig_tri.pdf}
\caption{Performance of the MR-MPI triangle-finding algorithm (Figure~\ref{fig:tri}).}
\label{f:tri}
\end{figure}

\subsection{Connected Components}

We compare our MR-MPI algorithm (Figure~\ref{fig:cc}) 
with a hybrid ``Giant Connected Component''
implementation using Trilinos and MR-MPI.  Power law graphs often have one
or two very large components and many very small components.  
The hybrid algorithm exploits this feature of the data by using inexpensive
breadth-first
search from the vertex with highest degree to identify the largest components,
followed by a more expensive algorithm to identify the small components in
the remainder of the graph.  In our hybrid implementation, we use
Trilinos' matrix-vector multiplication to do the breadth-first search, 
finding components that include (in total) 70\% or more of the vertices.
We then apply our MR-MPI algorithm to the remaining graph to identify
the small components.  This hybrid approach is extremely effective,
reducing the execution time to identify all components of RMAT-20 and 
RMAT-24 by 80-99\%, as shown in Figure~\ref{f:cc}.  The benefit of MR-MPI
is seen, however, for RMAT-28, where the problem is too large to fit into
memory for our hybrid algorithm.  

KDD NOTE:  Need to add XMT results.

\begin{figure}[htb]
\includegraphics[width=\textwidth]{fig_cc.pdf}
\caption{Performance of the MR-MPI connected components algorithm (Figure~\ref{fig:cc}) compared with a hybrid ``Giant Connected Component'' algorithm based
on Trilinos.}
\label{f:cc}
\end{figure}

\subsection{Maximally independent set results}

The execution times for the maximally independent set algorithm
(Figure~\ref{fig:luby}) are shown in Figure~\ref{f:luby}.  Like the R-MAT
generation results, superlinear speed-up of the algorithm occurs for RMAT-20
and RMAT-24, as more of the graph fits into processor memory and less file
I/O is needed.  For RMAT-28, the algorithm requires significant out-of-core
operations. In this case, parallel efficiency is nearly perfect going from 
8 to 32 processors; some loss of efficiency is seen at 64 processors,
where the parallel efficiency compared to the 8-processor case is 69\%.

KDD NOTE:  I think I had a bad node for the 64-processor run; I am 
rerunning it now and soon will have revised RMAT-28 results on 64 processors.

KDD NOTE:  Need to add XMT results; PBGL would be nice, too.

\begin{figure}[htb]
\includegraphics[width=\textwidth]{fig_luby.pdf}
\caption{Performance of maximally independent set algorithm (Figure~\ref{fig:luby}).}
\label{f:luby}
\end{figure}

\subsection{Single-source shortest path results}

The execution times for the single-source shortest path algorithms
(Figures~\ref{fig:sssp} and \ref{fig:sssp2})
are shown in Figure~\ref{f:sssp}.  The results show the benefit of
using the enhanced algorithm, providing at least a 17\% (and often greater)
reduction in execution time due to reduced communication; only the updated
distances are communicated throughout most of the enhanced algorithm.
However, the execution times are still large compared to multi-threaded
implementations; for example, Madduri et al.~\cite{Madduri07} report
execution times of only 11 seconds on 40 
processors for a multithreaded implementation on a Cray MTA-2
(the predecessor of the XMT) on graphs with one billion edges.

\begin{figure}[htb]
\includegraphics[width=\textwidth]{fig_sssp.pdf}
\caption{Execution times for SSSP using MR-MPI with R-MAT matrices.  
Both the original algorithm (Figure~\ref{fig:sssp}) and the enhanced 
algorithm (Figure~\ref{fig:sssp2}) are included.}
\label{f:sssp}
\end{figure}

KDD NOTE:  Need to add XMT results; PBGL would be nice, too.

To compare our MR-MPI implementation with a wider set of implementations,
we performed experiments MR-MPI, Hadoop, PBGL (Parallel
Boost Graph Library)~\cite{PBGL} and a multi-threaded implementation on the
Cray XMT using two web graphs: {WebGraphA} with 13.2M
vertices and 31.9M edges, and {WebGraphB} with 187.6M vertices and 531.9M edges.
In Figure~\ref{f:ssspA}, we show execution times for the SSSP algorithm
using {WebGraphA}.   Like our MR-MPI implementation, the Hadoop 
implementation is a Bellman-Ford-style~\cite{Bellman58,Ford62}  algorithm.  
The XMT and PBGL implementations are based on 
delta-stepping~\cite{MeyerSanders98}, and do not require full iterations
over the entire edge list to advance a breadth-first search.  
We observe that the MR-MPI
implementation runs in less time than the Hadoop implementation,but requires
significantly more time than the XMT and PBGL implementations.  
In experiments with {WebGraphB},
the benefit of the enhanced algorithm (Figure~\ref{fig:sssp2}) is clearly
shown, with a 40\% reduction in execution time compared to the original
SSSP algorith (Figure~\ref{fig:sssp}).
But the Bellman-Ford-style iterations are especially harmful to the MR-MPI and
Hadoop implementations for {WebGraphB}, which required 110 iterations to 
complete; execution times for this data set are show in Table~\ref{t:ssspB}.

\begin{figure}[htb]
\includegraphics[width=\textwidth]{fig_ssspA.pdf}
\caption{SSSP using MR-MPI for WebGraphA with
13.2M vertices and 31.9M edges.  Runtimes using Hadoop and PBGL
are also shown.}
\label{f:ssspA}
\end{figure}

\begin{table}
\begin{center}
\begin{tabular}{|l|c|r|}
\hline
Implementation & Number of Processes & SSSP Execution Time \\
\hline
Hadoop & 48  & 38,925 secs.\\
MR-MPI original (Figure~\ref{fig:sssp}) & 48 &  13,505 secs.\\
MR-MPI enhanced (Figure~\ref{fig:sssp2}) & 48 &  8,031 secs.\\
XMT/C  & 32 &  37 secs.\\
%Out-of-core MR-MPI & 96 &   8,358 secs.\\
%Out-of-core MR-MPI & 64 &  12,882 secs.\\
%Out-of-core MR-MPI & 100 &  6,280 secs.\\
\hline
\end{tabular}
\caption{Execution times for SSSP with {WebGraphB}.}
\label{t:ssspB}
\end{center}
\end{table}



\subsection{Scalability to large numbers of processors}
Finally, we demonstrate the scalability of MR-MPI to large numbers of 
processors.  
The MR-MPI implementation was run on Sandia's Redstorm and Thunderbird
parallel computers.  Redstorm is a massively parallel computer with 2+ 
{GHz} dual/quad-core AMD Opteron processors and 9.6 GB/s link bandwidth.
Thunderbird is a cluster with 3.6 {GHz} dual-core Intel EM64T processors
and an Infiniband network.
Because these systems do not have local disks for each processor, we selected 
a data set and page sizes that fit in memory, so out-of-core operations
were not used.  For these experiments, we used an R-MAT data set with 
with $2^{25}$ vertices and $2^{28}$ edges, with parameters given in
Table~\ref{t:rmat}.  We ran both PageRank and Connected Components
algorithms.

\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
Data & R-MAT  & R-MAT  & R-MAT  & R-MAT  & \# of    & \# of & Maximum \\
Set  & a      & b      & c      & d      & vertices & edges & vertex degree\\
\hline
nice  & 0.45 & 0.15 & 0.15 & 0.25 & $2^{25}$ & $2^{28}$ & 1108 \\
nasty & 0.57 & 0.19 & 0.19 & 0.05 & $2^{25}$ & $2^{28}$ & 230,207\\
\hline
\end{tabular}
\caption{Characteristics of R-MAT input data for PageRank and Connected
Components scalability experiments.}
\label{t:rmat}
\end{center}
\end{table}

In Figure~\ref{f:prbig}, we show the performance 
of the various PageRank
implementations on distributed memory and multi-threaded architectures.
The MR-MPI implementation demonstrated good scalability up to 1024 processors; 
however, it required an order-of-magnitude
more execution time than the matrix-based implementations on Redstorm.  
The distributed memory matrix-based
implementations are competitive with the multi-threaded implementation
in MTGL on the Cray XMT.

\begin{figure}[htb]
\includegraphics[width=\textwidth]{fig_pagerank_big.pdf}
\caption{Scalability comparison of PageRank implementations using MapReduce,
Trilinos, and MTGL on R-MAT data sets.}
\label{f:prbig}
\end{figure}

Similar results were obtained for the Connected Components algorithm, as
we show in Figure~\ref{f:ccbig}.  As with PageRank, the MR-MPI implementation
showed good scalability up to 1024 processors, but required significantly
more time than the Giant Connected Components algorithms using MTGL and
Trilinos.

\begin{figure}[htb]
\includegraphics[width=\textwidth]{fig_cc_big.pdf}
\caption{Scalability comparison of Connected Components implementations using MapReduce,
Trilinos, and MTGL on R-MAT data sets.}
\label{f:ccbig}
\end{figure}

