\section{MapReduce in MPI}
\label{sec:mr}

The basic datums stored and operated on by any MapReduce framweork are
key/value (KV) pairs.  In the MR-MPI library, individual keys or
values can be of any data type or length, or combinations of multiple
types (one integer, a string of characters, two integers and a double,
etc); they are simply treated as byte strings by the library.  A KV
pair always has a key; its value may be NULL.  A related data type is
the key/multivalue (KMV) pair, where all values associated with the
same key are collected and stored contiguously as a multivalue, which
is just a longer byte string with an associated vector of integer
lengths, one per value.

A typical MR-MPI program makes at least 3 calls to the MR-MPI library,
to perform a {\it map()}, {\it collate()}, and {\it reduce()}
operation.  In a {\it map()}, zero or more key/value pairs are
generated by each processor.  Typically, this is done using data read
in from files, but a {\it map()} may generate data itself or process
existing KV pairs to create new ones.  The KV pairs produced are
stored locally by each processor; a {\it map()} thus requires no
inter-processor communication.  Users call the library with a count of
tasks to perform and a pointer to a user function; the MR-MPI {\it
map()} operation invokes the user function multiple times as a
callback.  Depending on which variant of {\it map()} is called, the
user function may be passed a file name, a chunk of bytes from a large
file, or a task ID.  Options for assigning map tasks to processors.

The {\it collate()} operation (or data shuffle in Hadoop) identifies
unique keys and collects all the values associated with those keys to
create KMV pairs.  This is done in two stages, the first of which
requires communication, since KV pairs with the same key may be owned
by any processor.  Each processor hashes each of its keys to determine
the processor who ``owns'' it.  The $N$-byte length key is hashed into
a 32-bit value whose remainder modulo $P$ generates the owning
processor ID.  Alternatively, the user can provide a hash function
which converts the key into a processor ID.  Each processor then sends
each of its KV pairs to the owning processor.

After receiving new KV pairs, the second stage is an on-processor
computation, requiring no further communication.  Each processor
reorganizes its KV pairs into KMV pairs, one for each unique key it
owns.  This is done using a hash table, rather than a sort.  A sort
requires $\log(N)$ passes through the $N$ KV pairs.  the list of KMV
pairs can be created in 2 passes through the KV pairs, one to populate
the hash table with needed count and length information, and the
second pass to copy the key and value datums into the appropriate
location in a new KMV data structure.  Since the lookup of a key in a
well-formed hash table is a constant-time $O(1)$ operation, the cost
of the data reorganization is $O(N)$, rather than $N\log(N)$ for full
sort.  It also has the added benefit that the number of values in each
KMV pair is known, which is passed to the user function during a
reduce.  For some reduce operations, this is all the information a
reduce requires; thus the values need not be looped over.  This is not
the case for a Hadoop-style data reorg which sorts the values.  It
does not know a priori how many values are associated with the same
key without iterating over them.

Note that the first portion of the {\it collate()} operation involves
all-to-all communication (each processor sends and receives data from
every other processor) driven by a distributed hash table.  The
communication can either be done via a MPI\_Alltoall() library call,
or by a custom routine that aggregates messages and invokes pointwise
MPI\_Send() and MPI\_IRecv() calls.

The {\it reduce()} operation processes KMV pairs and can produce new
KV pairs for continued computation.  Each processor operates only on
the KMV pairs it owns; no communication is required.  As with the {\it
map()}, users call the library with a pointer to a user function.  The
MR-MPI {\it reduce()} operation invokes the user function, once for
each KMV pair.

Several related MapReduce operations are provided by the library.  For
example, the {\it compress()} operation combines on-processor KV pairs
to eliminate duplicate keys, forming new KV pairs with an aggregated
value.  The { \it convert()} operation turns a list of KV pairs into
KMV pairs, with one value per key.  The {collapse()} operation turns
$N$ KV pairs into one KMV pair, with the keys and values of the KV
pairs becoming $2N$ values of a single multivalue assigned to a new
key on the KMV.  The {\it gather()} operation collects KV pairs from
all processors to a subset of processors; it is useful for doing
output from a single or subset of processors.  Library calls for
sorting datums by key or value or within multivalues are also
provided.  These routines invoke the C-library quicksort() function to
compute the sorted ordering of the KV pairs (or values within a
multivalue), using a user-provided comparison function.  The KV pairs
(or values in a multivalue) are then copied one-by-one into a new data
structure in sorted order.

The purpose of providing an interface to various lo-level operations,
is that a user program can string them together in various ways to
produce interesting MapReduce algorithms.  For example, output from a
{\it reduce()} can serve as input to a subsequent {\it map()} or {\it
collate()}.  KV pairs from mutiple MapReduce objects can be combined
together to perform new sequences of {\it map()}, {\it collate()}, and
{\it reduce()} operations.

The above discussion assumed that the KV or KMV pairs stored by a
processor fit in its physical memory.  If this is the case, then we
refer to this as ``in-core'' processing and no disk files are written
or read by any of the processors, aside from initial input data if it
exists or final output data if it is generated.  Note that the
aggregate physical memory of large parallel machines can be multiple
Tbytes, which allows for large data sets to be processed in-core,
assuming the KV and KMV pairs remain evenly distributed across
processors throughout the sequence of MapReduce operations.  The use
of randomized hashing to assign keys to processors is designed for
such load-balancing.  What happens when data sets do not
fit in available memory, is the subject of the next section.
