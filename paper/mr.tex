\section{MapReduce in MPI}
\label{sec:mr}


  keys and values, KV and KMV data structures
  map and reduce are naturally parallel on distributed KV, KMV
    user owns data, just set of bytes passed to library
    flavors of map()
  collate is equivalant to all2all() on distributed hash table
    how we do irregular communication
  big difference is we are doing the comm in syncronous manner
    data is reorganized locally, once it is all on-proc
  other related operations: compress, gather, collapse, sort

MR-MPI~\cite{MRMPIManual} is a C++ library using MPI for
inter-processor communication.  The basic data units in MapReduce are
key/value pairs.  The data in keys and values can have any type or
length; they are stored in simple character arrays.  A related data
type is the key/multivalue pair, where all values associated with the
same key are collected and stored together as a multivalue.

The three basic operations in a typical MR-MPI program are {\it
map()}, {\it collate()}, and {\it reduce()}.  The {\it map()}
operation generates key/value pairs on each processor.  Typically,
this is done from data read in from files, but a {\it map()} may
generate data itself or process existing key/value pairs to generate
new ones.  The key/value pairs produced are stored locally on each
processor; a {\it map()} requires no inter-processor communication.
Users provide a pointer to their own map function; the MR-MPI {\it
map()} operation invokes the user function as a callback.

The {\it collate()} operation identifies unique keys and collects all
the values associated with those keys to create key/multivalue pairs.
This is done in two stages, the first of which requires communication,
since key/value pairs with the same key may be owned by any processor.
Each processor hashes its keys to an ``owning'' processor.  It then
sends each of its key/value pairs to the processor which owns the key.
After receiving key/value pairs, the second stage is an on-processor
computation, whereby each processor constructs key/multivalue pairs
for each unique key it now owns, using a local hash table.  Thus the
overall {\it collate()} is equivalent to an all-to-all communication
using a distributed hash table followed by a local reordering of the
data.  We use synchronous point-to-point irregular communication
instead of the MPI\_Alltoall() library call, since this allows control
of message sizes and sequencing.

The {\it reduce()} operation processes key/multivalue pairs and can
produce new key/value pairs for continued computation.  Each processor
operates on the key/multivalue pairs it owns; no communication is
required.  As with the {\it map()}, users provide pointers to their
own reduce functions.  The MR-MPI {\it reduce()} invokes the user's
function once for each key/multivalue pair.  Output from a {\it
reduce()} operation can serve as input to a subsequent {\it map()} or
{\it collate()}.

Several related operations are provided by MR-MPI.  For example, a
{\it compress()} combines on-processor key/value pairs to eliminate
duplicate keys, forming single key/value pairs with an aggregated
value.  The {\it gather()} operation collects key/values pairs from
all processors to a subset of processors; it is useful for doing
output from a single or subset of processors.  Library calls for
sorting data by key or value or within multivalues are also provided.

Since the MapReduce paradigm was designed to enable processing of very
large data sets, we provide some details of how MR-MPI manages memory
and large disk files.  The user (optionally) specifies a {\it memsize}
parameter when instantiating a MapReduce object, e.g. 1 Gbyte.  This
amount of memory is allocated by each processor for storing key/value
and key/multivalue pairs, as well as communication buffers and a hash
table for the {\it collate{}} operation.

If the data generated by various MR-MPI operations can be stored by
each processor within its allocated chunk of memory, we refer to this
as ``in-core'' processing and no disk files are written or read (aside
from initial input data if it exists or final output data if
generated).  Three additional points are worth noting.

At some stages in a MapReduce algorithm, the MapReduce object will
typically need to store both key/value and key/multivalue pairs at the
same time.  For example, during a {\it reduce()} operation,
key/multivalue pairs serve as input to the user's reduce function,
while the user's function may be generating new key/value pairs.

The memory available for storing key/value and key/multivalue pairs is
{\it memsize} summed across all processors.  So long as the key/value
pairs remain reasonably evenly distributed across processors then this
can be a significant amount of memory, e.g. 1 Tbyte on a
1024-processor machine.  The hash function used by the {\it collate()}
operation attempts to randomize key assignment to processors so
effective load-balance occurs.

For some MapReduce algorithms, it is convenient to create multiple
MapReduce objects and pass data back and forth between them.
Obviously, the user must then partition physical memory judiciously
between these various objects if all the data is to stay in-core.  The
{\it memsize} parameter can be specified for each MapReduce object
created.
