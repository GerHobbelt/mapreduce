MAP:  Generates, on each processor, 2^n words in [0, 2^n-1].
   When double p, total number of words also doubles (as in weak scaling).
   Range of words does not double, however, making other operations
   not truly a doubling (as in weak scaling). 

-  Map times for variable n, m and p = 1, 2, 4 look good.
   For a given n and m, map time should not vary with p.
   When n and m are doubled, map time should double.
   +  Don't know what is happening with p = 8.

-  Don't know what is happening for map when p=1, n=23, m varies.
   Why should the map time increase?

-  In map, did Steve have i%np or (i+me)%np?   (The latter is 
   easier to analyze, but I see I committed the former by accident.)

AGGREGATE:  KVs have key = processor number.  
   Number of KVs with a given key is 2^n.  
   Number of unique keys = p.

CONVERT:  For given n, # of KV per processor is 2^n.
   # KMV generated is 1 per processor (for key = processor).
   nvalue in MV is 2^n.
   convert() does not do communication.
   For given n and m, convert time should not vary with p.

REDUCE1:  Emit words in KMV.
   Each KMV has 2^n values.
   One KMV per processor.
   No communication.
   For given n and m, reduce1 time should not vary with p.

COMPRESS:  Steve's tests used i%np in the MAP phase -- my fault.  So the
   compress is a little harder to analyze.
   Each processor has 2^n KV.
   Each processor has 2^n/p unique keys.
   Each processor has p instances of each key.
   
COLLATE:  Each processor has 2^n/p unique keys.
   Each unique key has one value.
   Keys are not duplicated across processors, due to the way they were 
   mapped to processors by the map.
   It is almost certain, however, that collate() moves keys to new processors
   based on the hash function, so much communication could occur.
   Note that for p=1, this collate is not needed; the compress completes
   the word count.
   
REDUCE2:  Each processor should have approximately 2^n/np KMV, depending
   on the robustness of the hash function.  
   Each KMV should have only one nvalue (due to the mapping of keys to 
   processors in map()).
   Time differences could occur if the hash function did not evenly
   distribute the keys. 
   But otherwise, for a given n, the reduce2 function should exhibit strong
   scaling with increasing p; that is, doubling p should cut the reduce2
   time in half.
   Note that for p=1, this reduce is not needed; the compress completes
   the word count.
    

